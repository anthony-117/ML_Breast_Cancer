install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
library("tidyverse")
library(tidyverse)
library(tidyverse)
tidyverse
tidyverse_logo()
clear()
cls()
cls
clear
install.packages("tidyverse")
library(tidyverse)
mpg
tidyverse::mpg
install.packages("tidyverse")
library(tidyverse)
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
library()
library(tidyverse)
library(tidyverse)
library(tidyverse)
mpg
ggplot2
ggplot2()
library(tidyverse)
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
?mpg
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = class)
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = class)
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = class)
)
view(mpg)
ggplot(data = mpg)
?mpg
ggplot(data = mpg) +
geom_point(mapping = aes(x=hwy, y = cyl))
geom_point(mapping = aes(y=hwy, x = cyl))
geom_point(mapping = aes(x = cyl, y = hwy))
geom_point(mapping = aes(y=hwy, x = cyl))
geom_point(mapping = aes(x=hwy, y = cyl))
ggplot(data = mpg) +
geom_point(mapping = aes(x= cyl, y = hwy))
ggplot(data = mpg) +
geom_point(mapping = aes(x= class, y = drv))
?mpg
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy), color = class)
geom_point(mapping = aes(x = displ, y = hwy, color = class))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = class))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = displ < 5))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy)) +
geom_smooth(mapping = aes(x = displ, y = hwy))
ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +
geom_point() +
geom_smooth(se = FALSE)
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +
geom_point() +
geom_smooth()
ggplot() +
geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) +
geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy))
summary(mpg)
ggplot::diamonds
?ggplot2::diamonds
anthony <- 2 ^ 3
anthony
library(tidyverse)
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
fliter(mpg, cyl = 8)
library(tidyverse)
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
filter(mpg, cyl = 8)
?mpg
library(tidyverse)
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
filter(mpg, cyl == 8)
filter(mpg, cyl < 3)
filter(mpg, mpg$cyl < 3)
ls()
?ls
filter(mpg, cyl == 8)
data(mpg)
filter(mpg, cyl == 8)
?mpg
filter(diamonds, carat > 3)
data(carat)
data(diamonds)
filter(mpg, cyl == 8)
filter(diamonds, carat > 3)
filter(diamonds, carat > 3)
rm()
rm(list = ls())
library(nycflights13)
install.packages("nycflights13")
libarary(nycflights13)
library(nycflights13)
library(tidyverse)
?nycflights13
??nycflights13
search()
sessioninfO()
sessionInfo(
)
loadedNamespaces()
flights
view(flights)
filter(flights, month == 1, day == 1)
filter(flights, flights$month == 1, flights$day == 1)
?dplyr
colnames(flights)
str(flights)
library(nycflights13)
filter(flights, month == 1, day == 1)
library(dplyr)
library(tidyverse)
library(tidyverse)
install.packages("stringi")
install.packages("stringi")
library(tidyverse)
source("~/.active-rstudio-document")
# Run the example
example_usage()
View(NeuralNetwork)
# Create a simple XOR problem
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE)
y <- matrix(c(0, 1, 1, 0), ncol = 1)
View(X)
View(y)
# Create a neural network with 2 input neurons, 4 hidden neurons, and 1 output neuron
nn <- NeuralNetwork(input_size = 2, hidden_size = 4, output_size = 1, learning_rate = 0.1)
# Train the neural network
cost_history <- nn$train(X, y, epochs = 10000, verbose = TRUE)
X <- as.matrix(X)
y <- as.matrix(y)
cost_history <- numeric(epochs)
for (epoch in 1:epochs) {
# Forward pass
forward_result <- forward(X)
# Calculate error
error <- y - forward_result$output
cost <- mean(error^2)
cost_history[epoch] <- cost
if (verbose && epoch %% 100 == 0) {
cat("Epoch:", epoch, "Cost:", cost, "\n")
}
# Backward propagation
# Output layer gradients
delta_output <- error * sigmoid_derivative(forward_result$output_input)
# Hidden layer gradients
delta_hidden <- (delta_output %*% t(weights_hidden_output)) * sigmoid_derivative(forward_result$hidden_input)
# Update weights and biases (batch gradient descent)
# For hidden to output weights
weights_hidden_output_gradient <- t(forward_result$hidden_output) %*% delta_output
bias_output_gradient <- colSums(delta_output)
# For input to hidden weights
weights_input_hidden_gradient <- t(X) %*% delta_hidden
bias_hidden_gradient <- colSums(delta_hidden)
# Apply gradients with learning rate
weights_hidden_output <<- weights_hidden_output + learning_rate * weights_hidden_output_gradient
bias_output <<- bias_output + learning_rate * bias_output_gradient
weights_input_hidden <<- weights_input_hidden + learning_rate * weights_input_hidden_gradient
bias_hidden <<- bias_hidden + learning_rate * bias_hidden_gradient
}
# Input to hidden layer
hidden_input <- X %*% weights_input_hidden + matrix(rep(bias_hidden, nrow(X)), nrow = nrow(X), byrow = TRUE)
# Initialize the neural network with random weights
weights_input_hidden <- matrix(runif(input_size * hidden_size, -1, 1), nrow = input_size, ncol = hidden_size)
weights_hidden_output <- matrix(runif(hidden_size * output_size, -1, 1), nrow = hidden_size, ncol = output_size)
bias_output <- matrix(runif(output_size, -1, 1), nrow = 1, ncol = output_size)
# Forward propagation
forward <- function(X) {
# Input to hidden layer
hidden_input <- X %*% weights_input_hidden + matrix(rep(bias_hidden, nrow(X)), nrow = nrow(X), byrow = TRUE)
hidden_output <- sigmoid(hidden_input)
# Hidden to output layer
output_input <- hidden_output %*% weights_hidden_output + matrix(rep(bias_output, nrow(X)), nrow = nrow(X), byrow = TRUE)
output <- sigmoid(output_input)
return(list(
hidden_input = hidden_input,
hidden_output = hidden_output,
output_input = output_input,
output = output
))
}
rm(list =  = ls())
rm(list  = ls())
# Create a simple XOR problem
X <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE)
y <- matrix(c(0, 1, 1, 0), ncol = 1)
input_size <- 2
hidden_size <- 4
output_size <- 1
learning_rate <- 0.1
# Initialize the neural network with random weights
weights_input_hidden <- matrix(runif(input_size * hidden_size, -1, 1), nrow = input_size, ncol = hidden_size)
bias_hidden <- matrix(runif(hidden_size, -1, 1), nrow = 1, ncol = hidden_size)
weights_hidden_output <- matrix(runif(hidden_size * output_size, -1, 1), nrow = hidden_size, ncol = output_size)
bias_output <- matrix(runif(output_size, -1, 1), nrow = 1, ncol = output_size)
verbose <- TRUE
epochs <- 10
# Input to hidden layer
hidden_input <- X %*% weights_input_hidden + matrix(rep(bias_hidden, nrow(X)), nrow = nrow(X), byrow = TRUE)
View(hidden_input)
View(weights_input_hidden)
hidden_output <- sigmoid(hidden_input)
# Hidden to output layer
output_input <- hidden_output %*% weights_hidden_output + matrix(rep(bias_output, nrow(X)), nrow = nrow(X), byrow = TRUE)
hidden_output <- sigmoid(hidden_input)
# Sigmoid activation function and its derivative
sigmoid <- function(x) {
return(1 / (1 + exp(-x)))
}
sigmoid_derivative <- function(x) {
sx <- sigmoid(x)
return(sx * (1 - sx))
}
hidden_output <- sigmoid(hidden_input)
# Hidden to output layer
output_input <- hidden_output %*% weights_hidden_output + matrix(rep(bias_output, nrow(X)), nrow = nrow(X), byrow = TRUE)
View(output_input)
View(weights_input_hidden)
View(weights_hidden_output)
# Input to hidden layer
hidden_input <- X %*% weights_input_hidden + matrix(rep(bias_hidden, nrow(X)), nrow = nrow(X), byrow = TRUE)
View(hidden_input)
# Input to hidden layer
hidden_input <- X %*% weights_input_hidden + matrix(rep(bias_hidden, nrow(X)), nrow = nrow(X), byrow = TRUE)
hidden_output <- sigmoid(hidden_input)
# Hidden to output layer
output_input <- hidden_output %*% weights_hidden_output + matrix(rep(bias_output, nrow(X)), nrow = nrow(X), byrow = TRUE)
output <- sigmoid(output_input)
View(output)
setwd("C:\\Users\\antho\\Desktop\\ML\\project2")
source("dataset_creation.R")
source("NN_simple.R")
source("plots.R")
dataset <- read.csv("sine_wave_dataset_preprocess.csv")
raw_dataset <- read.csv("sine_wave_dataset.csv")
dataset <- dataset %>%
filter(X < 3 & X > -3)
X <- as.matrix(dataset$X)
Y <- as.matrix(dataset$Y)
nn <- NN.create(X, Y, c(30, 20, 10))
epochs <- 10000
# # result <- NN.train(nn, X, Y, epochs = epochs, learning_rate = 0.01, momentum = 0.9)
# result <- NN.train(nn, X, Y, epochs=epochs, learning_rate=0.01,
#                        batch_type="mini", batch_size = 64, optimizer = "adagrad", verbose=TRUE)
result <- NN.train(nn, X, Y, epochs=10000, learning_rate=0.01, momentum=0.2,
batch_type="mini", batch_size=20, verbose=TRUE)
NN.predict.periodic(NN, seq(10, 20, by = 0.1))
NN.predict.periodic(nn, seq(10, 20, by = 0.1))
NN.predict.periodic(nn, seq(10, 20, by = 0.1))
X <- (X.predict %% 2)  - 1
Y <- NN.predict(NN, X)
library(ggplot2)
NN.predict.periodic(nn, seq(10, 20, by = 0.1))
X.predict <- X
NN <- nn
X <- (X.predict %% 2)  - 1
Y <- NN.predict(NN, X)
df <- data.frame(
X = as.numeric(X.predict),
Predictions = as.numeric(Y)
)
df <- df[order(df$X), ]
# Real-time plot
p <- ggplot(df, aes(x = X)) +
# geom_line(aes(y = True_Values, color = "True Values"), size = 1) +
# *** Solid blue line for predictions ***
geom_line(aes(y = Predictions, color = "Predictions"),
size = 1, linetype = "solid") +
scale_color_manual(values = c("Predictions" = "tomato")) +
labs(title = paste0("Epoch ", epoch, " | Cost: ", signif(epoch_cost, 4)),
x = "X", y = "Y") +
theme_minimal() +
theme(legend.title = element_blank())
# Real-time plot
p <- ggplot(df, aes(x = X)) +
# geom_line(aes(y = True_Values, color = "True Values"), size = 1) +
# *** Solid blue line for predictions ***
geom_line(aes(y = Predictions, color = "Predictions"),
size = 1, linetype = "solid") +
scale_color_manual(values = c("Predictions" = "tomato")) +
labs(title = "Predictions",
x = "X", y = "Y") +
theme_minimal() +
theme(legend.title = element_blank())
print(p)
NN.predict.periodic(nn, seq(10, 20, by = 0.1))
source("NN_simple.R")
NN.predict.periodic(nn, seq(10, 20, by = 0.1))
load("128x64x32_adam_mini.RData")
nn <- result$NN
NN.predict.periodic(nn, seq(10, 20, by = 0.1))
X.predict <- seq(10, 20, by = 0.1)
NN.predict(nn, X.predict)
NN.predict.periodic(nn, X.predict)
source("NN_simple.R")
# result <- NN.train(nn, X, Y, epochs=1000, learning_rate=0.1, momentum=0.9,
#                    batch_type="stochastic", batch_size=20, verbose=TRUE)
NN.trained <- result$NN
X.predict <- seq(10, 20, by = 0.1)
X.predict <- seq(10, 20, by = 0.1, verbose = TRUE)
X.predict <- seq(10, 20, by = 0.1)
NN.predict(nn, X.predict, verbose = TRUE)
source("NN_simple.R")
NN.predict.periodic(nn, X.predict)
source("NN_simple.R")
NN.predict.periodic(nn, X.predict)
traceback()
source("NN_simple.R")
NN.predict.periodic(nn, X.predict)
source("NN_simple.R")
NN.predict.periodic(nn, X.predict)
source("dataset_creation.R")
source("NN_simple.R")
NN.predict.periodic(nn, X.predict)
source("NN_simple.R")
NN.predict.periodic(nn, X.predict)
View(scale)
View(scale)
View(scale)
View(scale)
View(scale)
View(scale)
View(scale)
X <- (X.predict %% 2)  - 1
Y <- NN.predict(NN, X)
y.true <- calculate_y(X.predict)
y.true <- scale(y.true)
scale <- function(X){
result <- (X - min(.)/(max(.) - min(.)))
return(result)
}
y.true <- scale(y.true)
scale <- function(X){
result <- (X - min(.)/(max(.) - min(.)))
return(result)
}
X <- (X.predict %% 2)  - 1
Y <- NN.predict(NN, X)
y.true <- calculate_y(X.predict)
y.true <-
df <- data.frame(
X = as.numeric(X.predict),
Predictions = as.numeric(Y),
True_Values = as.numeric(y.true)
)
df <- df[order(df$X), ]
y.true <- scale(y.true)
y.true <- calculate_y(X.predict)
y.true <- scale(y.true)
typeof(y.true)
View(y.true)
y.true <- scale(X)
y.true <- as.numeric(y.true)
y.true <- scale(X)
y.true <- scale(y.true)
true <- calculate_y(X.predict)
true <- scale(true)
is.na(y.true)
any(is.na(y.true))
scaley <- function(X){
result <- (X - min(.)/(max(.) - min(.)))
return(result)
}
true <- scaley(true)
true <- calculate_y(X.predict)
true <- scaley(true)
scaley <- function(X){
result <- (X - min(.)/(max(.) - min(.)))
return(result)
}
X <- (X.predict %% 2)  - 1
Y <- NN.predict(NN, X)
true <- calculate_y(X.predict)
true <- scaley(true)
scale <- function(X) {
min_val <- min(X)
max_val <- max(X)
scaled_X <- (X - min_val) / (max_val - min_val)
return(scaled_X)
}
unscale <- function(y, min_val, max_val) {
unscaled_X <- (y * (max_val - min_val)) + min_val
return(unscaled_X)
}
true <- scale(true)
X <- (X.predict %% 2)  - 1
Y <- NN.predict(NN, X)
y.true <- calculate_y(X.predict)
y.true <- scale(true)
df <- data.frame(
X = as.numeric(X.predict),
Predictions = as.numeric(Y),
True_Values = as.numeric(y.true)
)
df <- df[order(df$X), ]
# Real-time plot
# Real-time plot
p <- ggplot(df, aes(x = X)) +
geom_line(aes(y = True_Values, color = "True Values"), size = 1) +
# *** Solid blue line for predictions ***
geom_line(aes(y = Predictions, color = "Predictions"),
size = 1, linetype = "solid") +
scale_color_manual(values = c("True Values" = "skyblue", "Predictions" = "tomato")) +
labs(title = "Prediction",
x = "X", y = "Y") +
theme_minimal() +
theme(legend.title = element_blank())
print(p)
source("NN_simple.R")
NN.predict.periodic(nn, X.predict)
source("NN_simple.R")
source("plots.R")
dataset <- read.csv("sine_wave_dataset_preprocess.csv")
raw_dataset <- read.csv("sine_wave_dataset.csv")
NN.predict.periodic(nn, X.predict)
source("NN_simple.R")
NN.predict(nn, X.predict, verbose = TRUE)
load("128x64x32_adam_mini.RData")
nn <- result$NN
NN.predict(nn, X.predict, verbose = TRUE)
y <- as.matrix(X)
for (l in 1:length(NN$weights)) {
y.p <- cbind(-1, y)
z.l <- y.p %*% NN$weights[[l]]
y <- activation.fn(z.l)
}
source("NN_simple.R")
NN.predict(nn, X.predict, verbose = TRUE)
source("NN_simple.R")
NN.predict.periodic(nn, X.predict)
NN.predict(nn, X.predict, verbose = TRUE)
X.predict <- seq(1, 2, by = 0.1)
NN.predict(nn, X.predict, verbose = TRUE)
X.predict <- seq(5, 10, by = 0.1)
NN.predict(nn, X.predict, verbose = TRUE)
load("30x20x10_sgd_mini.RData")
nn <- result$NN
X.predict <- seq(5, 10, by = 0.1)
NN.predict(nn, X.predict, verbose = TRUE)
source("NN_simple.R")
NN.predict(nn, X.predict, verbose = TRUE)
source("NN_simple.R")
NN.predict(nn, X.predict, verbose = TRUE)
X.predict <- seq(-5, -10, by = 0.1)
X.predict <- seq(-10, -5, by = 0.1)
NN.predict(nn, X.predict, verbose = TRUE)
NN.predict.periodic(nn, X.predict)
source("NN_simple.R")
X.predict <- seq(-10, -5, by = 0.1)
NN.predict(nn, X.predict, verbose = TRUE)
NN.predict.periodic(nn, X.predict)
X.predict <- seq(5, 10, by = 0.1)
NN.predict(nn, X.predict, verbose = TRUE)
NN.predict.periodic(nn, X.predict)
load("128x64x32_adam_mini.RData")
nn <- result$NN
X.predict <- seq(5, 10, by = 0.1)
NN.predict(nn, X.predict, verbose = TRUE)
NN.predict.periodic(nn, X.predict)
#
# # Reshape data for ggplot
# df_long <- df %>%
#   pivot_longer(cols = c("Y_Predicted", "Y_True"), names_to = "Type", values_to = "Y")
#
# # Plot
# ggplot(df_long, aes(x = X, y = Y, color = Type)) +
#   geom_line() +
#   labs(title = "Training Predictions vs True Values", x = "X", y = "Y") +
#   theme_minimal()
plot_cost_history(result, show_test = TRUE)
