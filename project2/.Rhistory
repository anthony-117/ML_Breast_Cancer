yaxis = list(title = "Prediction Value"))
fig
install.packages("shiny")
library(shiny)
ui <- fluidPage(
titlePanel("Neural Network Training Progress"),
sidebarLayout(
sidebarPanel(
sliderInput("epoch", "Select Epoch:", min = 1, max = 50, value = 1)
),
mainPanel(
plotOutput("plot")
)
)
)
server <- function(input, output) {
output$plot <- renderPlot({
epochs <- 1:50
true_values <- sin(epochs / 10)
predictions <- sin(epochs / 10) + rnorm(50, mean = 0, sd = 0.1)
plot(epochs, true_values, type = "l", col = "blue", lwd = 2, ylim = c(-1, 1),
xlab = "Epochs", ylab = "Prediction Value", main = "Training Progress")
points(epochs[1:input$epoch], predictions[1:input$epoch], col = "red", pch = 19)
lines(epochs[1:input$epoch], predictions[1:input$epoch], col = "red", lwd = 2)
legend("topright", legend = c("True Values", "Predictions"), col = c("blue", "red"), lty = 1, pch = 19)
})
}
shinyApp(ui = ui, server = server)
library(ggplot2)
library(gganimate)
install.packages("gganimate")
library(gifski)
install.packages("gifski")
install.packages("gifski")
setwd("Desktop/ML/ML_Breast_Cancer/project1")
getwd()
source("preprocess.R")
source("NN_new.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0, verbose = TRUE)
source("NN_new.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0, verbose = TRUE)
source("NN_new.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0, verbose = TRUE)
source("NN_new.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0, verbose = TRUE)
source("NN_new.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0, verbose = TRUE)
NN.trained <- result$NN
training_data <- result$training_data
plot_confusion_matrix(NN.trained, X, Y)
result <- NN.train(NN,X, Y, 10000, 0.01, 0.9, verbose = TRUE)
source("NN_new.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0.9, verbose = TRUE)
NN.trained <- result$NN
training_data <- result$training_data
plot_confusion_matrix(NN.trained, X, Y)
setwd("../project2/")
source("dataset_creation.R")
source("NN_new.R")
source("visualize_training_results.R")
dataset <- read.csv("sine_wave_dataset_preprocess.csv")
raw_dataset <- read.csv("sine_wave_dataset.csv")
X <- as.matrix(dataset$X)
X <- as.matrix(dataset$X)
X <- as.matrix(dataset$X)
X <- as.matrix(dataset$X)
Y <- as.matrix(dataset$Y)
nn <- NN.create(X, Y, c(5,4,3,2))
epochs <- 10000
result <- NN.train(nn, X, Y, epochs = epochs, learning_rate = 0.01, momentum = 0.9)
source("NN.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0.9, verbose = TRUE)
rm(list = ls())
source("preprocess.R")
setwd("../project1")
source("preprocess.R")
source("NN.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0.9, verbose = TRUE)
source("NN_new.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0.9, verbose = TRUE)
source("NN_new.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0.9, verbose = TRUE)
source("NN_new.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0.9, verbose = TRUE)
source("NN_new.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0.9, verbose = TRUE)
source("NN_new.R")
NN <- NN(X = X, Y = Y,
hidden_layers = c(5,3)
)
result <- NN.train(NN,X, Y, 10000, 0.01, 0.9, verbose = TRUE)
NN.trained <- result$NN
training_data <- result$training_data
plot_confusion_matrix(NN.trained, X, Y)
library(ggplot2)
# Define a range of learning rates
learning_rates <- c(0.001, 0.01, 0.1, 0.5)
# Train the network with different learning rates
training_results_lr <- NN.train_multiple.lr(NN, X, Y, epochs = 1000, learning_rates, momentum = 0.9, verbose = FALSE)
# Convert results into a dataframe for ggplot2
df_lr <- data.frame(Epoch = integer(), Cost = numeric(), Accuracy = numeric(), LearningRate = character())
for (lr in names(training_results_lr)) {
df_temp <- data.frame(
Epoch = 1:length(training_results_lr[[lr]]$cost_history),
Cost = training_results_lr[[lr]]$cost_history,
Accuracy = training_results_lr[[lr]]$accuracy_history,
LearningRate = lr
)
df_lr <- rbind(df_lr, df_temp)
}
# Plot Cost History
ggplot(df_lr, aes(x = Epoch, y = Cost, color = LearningRate)) +
geom_line(size = 1) +
labs(title = "Effect of Learning Rate on Cost",
x = "Epoch",
y = "Cost") +
theme_minimal()
# Plot Accuracy History
ggplot(df_lr, aes(x = Epoch, y = Accuracy, color = LearningRate)) +
geom_line(se = FALSE) +
labs(title = "Effect of Learning Rate on Accuracy",
x = "Epoch",
y = "Accuracy") +
theme_minimal()
# Define a range of momentum values
momentums <- c(0.5, 0.7, 0.9, 0.99)
# Train the network with different momentum values
training_results_momentum <- NN.train_multiple.momentum(NN, X, Y, epochs = 1000, learning_rate = 0.01, momentums, verbose = FALSE)
# Convert results into a dataframe for ggplot2
df_momentum <- data.frame(Epoch = integer(), Cost = numeric(), Accuracy = numeric(), Momentum = character())
for (m in names(training_results_momentum)) {
df_temp <- data.frame(
Epoch = 1:length(training_results_momentum[[m]]$cost_history),
Cost = training_results_momentum[[m]]$cost_history,
Accuracy = training_results_momentum[[m]]$accuracy_history,
Momentum = m
)
df_momentum <- rbind(df_momentum, df_temp)
}
# Plot Cost History
ggplot(df_momentum, aes(x = Epoch, y = Cost, color = Momentum)) +
geom_line(size = 1) +
labs(title = "Effect of Momentum on Cost",
x = "Epoch",
y = "Cost") +
theme_minimal()
# Plot Accuracy History
ggplot(df_momentum, aes(x = Epoch, y = Accuracy, color = Momentum)) +
geom_line() +
labs(title = "Effect of Momentum on Accuracy",
x = "Epoch",
y = "Accuracy") +
theme_minimal()
result <- NN.train(NN,X, Y, 10000, 0.01, 0.5, verbose = TRUE)
NN.trained <- result$NN
NN.trained <- result$NN
training_data <- result$training_data
plot_confusion_matrix(NN.trained, X, Y)
library(ggplot2)
df_acc <- data.frame(
epoch = 1:length(training_data$accuracy_history),
accuracy = training_data$accuracy_history,
cost = training_data$cost_history
)
ggplot(df_acc, aes(x = epoch, y = accuracy)) +
geom_line(color="blue")
# Create a smooth spline for the accuracy and cost data
accuracy_smooth <- smooth.spline(df_acc$epoch, df_acc$accuracy)
cost_smooth <- smooth.spline(df_acc$epoch, df_acc$cost)
# Plot the smoothed accuracy data
plot(accuracy_smooth, col = "skyblue",
xlab = "Epoch", ylab = "Accuracy",
main = "Accuracy and Cost Over Epochs")
# Add a second y-axis on the right side
par(new = TRUE)
# Plot the smoothed cost data with the second y-axis
plot(cost_smooth, col = "tomato", axes = FALSE, xlab = "", ylab = "")
# Add the second y-axis
axis(side = 4)
mtext("Cost", side = 4, line = 3)
# Add a legend
legend("topright", legend = c("Accuracy", "Cost"),
col = c("skyblue", "tomato"), lty = 1)
setwd("../project2/")
rm(list = ls())
source("dataset_creation.R")
source("NN_new.R")
source("visualize_training_results.R")
dataset <- read.csv("sine_wave_dataset_preprocess.csv")
raw_dataset <- read.csv("sine_wave_dataset.csv")
X <- as.matrix(dataset$X)
Y <- as.matrix(dataset$Y)
nn <- NN.create(X, Y, c(5,4,3,2))
epochs <- 10000
result <- NN.train(nn, X, Y, epochs = epochs, learning_rate = 0.01, momentum = 0.9)
epochs <- 1000
result <- NN.train(nn, X, Y, epochs = epochs, learning_rate = 0.01, momentum = 0.9)
NN.trained <- result$NN
# Plot the cost progression across all epochs
cost_progression_plot <- plot_cost_progression(result,
"Cost Progression")
print(cost_progression_plot)
# Plot predictions vs true values for a specific epoch (e.g., last epoch)
last_epoch <- epochs
plot(result$test_X[[last_epoch]], result$test_predictions[[last_epoch]],
col = "blue", pch = 16, main = "Predictions vs True Values")
points(result$test_X[[last_epoch]], result$test_true_values[[last_epoch]],
col = "red", pch = 16)
legend("topright", c("Predictions", "True Values"),
col = c("blue", "red"), pch = 16)
plot_last_epoch(result, "My Neural Network Performance")
source("NN_new.R")
source("visualize_training_results.R")
dataset <- read.csv("sine_wave_dataset_preprocess.csv")
raw_dataset <- read.csv("sine_wave_dataset.csv")
X <- as.matrix(dataset$X)
Y <- as.matrix(dataset$Y)
nn <- NN.create(X, Y, c(5,4,3,2))
epochs <- 1000
source("NN.R")
source("visualize_training_results.R")
dataset <- read.csv("sine_wave_dataset_preprocess.csv")
raw_dataset <- read.csv("sine_wave_dataset.csv")
X <- as.matrix(dataset$X)
Y <- as.matrix(dataset$Y)
nn <- NN.create(X, Y, c(5,4,3,2))
epochs <- 10000
result <- NN.train(nn, X, Y, epochs = epochs, learning_rate = 0.01, momentum = 0.9)
source("NN.R")
source("visualize_training_results.R")
dataset <- read.csv("sine_wave_dataset_preprocess.csv")
raw_dataset <- read.csv("sine_wave_dataset.csv")
X <- as.matrix(dataset$X)
Y <- as.matrix(dataset$Y)
nn <- NN.create(X, Y, c(5,4,3,2))
epochs <- 10000
NN <- nn
epochs <- 100
cost_history <- numeric(epochs)
# Lists to store predictions, true values, and corresponding X values for each epoch
train_predictions <- list()
test_predictions <- list()
train_true_values <- list()
test_true_values <- list()
train_X <- list()
test_X <- list()
best_weights <- NN$weights
min_cost <- Inf  # Initialize minimum cost as infinity
old_weight <- NN$weights
percent <- 0.8  # Training split percentage
View(NN$weights)
NN$weights[[4]]
old_weight <- NN$weights
percent <- 0.8  # Training split percentage
epoch <- 1
set.seed(40)
indices <- sample(1:nrow(X), size = percent * nrow(X))
X.train <- X[indices, ]
Y.train <- Y[indices, ]
X.test <- X[-indices, ]
Y.test <- Y[-indices, ]
View(X.train)
# Forward propagation
forward_result <- feed_forward(NN, X.train)
y.fw <- forward_result$y.fw
z.fw <- forward_result$z.fw
output <- y.fw[[length(y.fw)]]
View(z.fw)
View(y.fw)
View(output)
# Compute cost
error <- Y.train - output
current_cost <- cost(error)
cost_history[epoch] <- current_cost
# Store predictions, true values, and X values for training
train_predictions[[epoch]] <- output
train_true_values[[epoch]] <- Y.train
train_X[[epoch]] <- X.train
# Check if this is the best cost so far
if (current_cost < min_cost) {
min_cost <- current_cost
best_weights <- NN$weights
}
# Backpropagation
dirac <- back_propagation(NN, z.fw, Y.train)
View(dirac)
l <- 1
a <- cbind(-1, y.fw[[l]])
weight.delta <- learning_rate * (t(a) %*% dirac[[l]]) +
momentum * (NN$weights[[l]] - old_weight[[l]])
learning_rate <- 0.01
weight.delta <- learning_rate * (t(a) %*% dirac[[l]]) +
momentum * (NN$weights[[l]] - old_weight[[l]])
momentum <- 0.5
weight.delta <- learning_rate * (t(a) %*% dirac[[l]]) +
momentum * (NN$weights[[l]] - old_weight[[l]])
NN$weights[[l]] <- NN$weights[[l]] + weight.delta
View(weight.delta)
cost_history <- numeric(epochs)
# Lists to store predictions, true values, and corresponding X values for each epoch
train_predictions <- list()
test_predictions <- list()
train_true_values <- list()
test_true_values <- list()
train_X <- list()
test_X <- list()
best_weights <- NN$weights
source("NN_new.R")
nn <- NN.create(X, Y, c(5,4,3,2))
NN <- nn
NN$weights
source("NN_new.R")
source("visualize_training_results.R")
dataset <- read.csv("sine_wave_dataset_preprocess.csv")
raw_dataset <- read.csv("sine_wave_dataset.csv")
X <- as.matrix(dataset$X)
Y <- as.matrix(dataset$Y)
nn <- NN.create(X, Y, c(5,4,3,2))
nn$weights
source("NN.R")
nn <- NN.create(X, Y, c(5,4,3,2))
nn$weights
cost_history <- numeric(epochs)
# Lists to store predictions, true values, and corresponding X values for each epoch
train_predictions <- list()
test_predictions <- list()
train_true_values <- list()
test_true_values <- list()
train_X <- list()
test_X <- list()
best_weights <- NN$weights
NN <- nn
best_weights <- NN$weights
min_cost <- Inf  # Initialize minimum cost as infinity
old_weight <- NN$weights
percent <- 0.8  # Training split percentage
set.seed(40)
epoch
epoch
indices <- sample(1:nrow(X), size = percent * nrow(X))
View(indices)
X.train <- X[indices, ]
Y.train <- Y[indices, ]
X.test <- X[-indices, ]
Y.test <- Y[-indices, ]
# Forward propagation
forward_result <- feed_forward(NN, X.train)
y.fw <- forward_result$y.fw
z.fw <- forward_result$z.fw
output <- y.fw[[length(y.fw)]]
View(z.fw)
View(y.fw)
View(output)
# Compute cost
error <- Y.train - output
current_cost <- cost(error)
cost_history[epoch] <- current_cost
# Store predictions, true values, and X values for training
train_predictions[[epoch]] <- output
train_true_values[[epoch]] <- Y.train
train_X[[epoch]] <- X.train
# Check if this is the best cost so far
if (current_cost < min_cost) {
min_cost <- current_cost
best_weights <- NN$weights
}
# Backpropagation
dirac <- back_propagation(NN, z.fw, Y.train)
View(dirac)
l
a <- cbind(-1, y.fw[[l]])
weight.delta <- learning_rate * (t(a) %*% dirac[[l]]) +
momentum * (NN$weights[[l]] - old_weight[[l]])
View(weight.delta)
source("NN_new.R")
nn <- NN.create(X, Y, c(5,4,3,2))
NN <- nn
nn$weights
cost_history <- numeric(epochs)
# Lists to store predictions, true values, and corresponding X values for each epoch
train_predictions <- list()
test_predictions <- list()
train_true_values <- list()
test_true_values <- list()
train_X <- list()
test_X <- list()
best_weights <- NN$weights
min_cost <- Inf  # Initialize minimum cost as infinity
old_weight <- NN$weights
percent <- 0.8  # Training split percentage
indices <- sample(1:nrow(X), size = percent * nrow(X))
indices
X.train <- X[indices, ]
Y.train <- Y[indices, ]
X.test <- X[-indices, ]
Y.test <- Y[-indices, ]
# Forward propagation
forward_result <- forward_propagation(NN, X.train)
y.fw <- forward_result$y.fw
z.fw <- forward_result$z.fw
output <- y.fw[[length(y.fw)]]
z.fw
z.fw[[1]]
z.fw[[5]]
y.fw[[1]]
current_cost <- cost(y.predict = output, y.true = Y.train)
cost_history[epoch] <- current_cost
output
current_cost <- cost(y.predict = output, y.true = Y.train)
cost_history[epoch] <- current_cost
# Store predictions, true values, and X values for training
train_predictions[[epoch]] <- output
train_true_values[[epoch]] <- Y.train
train_X[[epoch]] <- X.train
# Check if this is the best cost so far
if (current_cost < min_cost) {
min_cost <- current_cost
best_weights <- NN$weights
}
Y <- Y.train
Y <- as.matrix(Y)
gradients <- list()
m <- nrow(Y)
m
nb.weights <- length(NN$weights)
output <- y.fw[[length(y.fw)]]
# dy -> partial derivative of cost with respect
# to y
dy <- cost.derivative(y.predict = output, y.true = Y)
dim(dy)
l
l <- nb.weights
l
length(y.fw)
weight <- NN$weights[[l]]
dz <- dy * activation.dfn(z.fw[[l]])
dz
dim(dz)
dz <- dy * activation.dfn(z.fw[[l]])
dz
y.prev <- cbind(-1,y.fw[[l]])
dW <- (t(y.prev) %*% dz) /m
dQ
dQ
dW
dim(y.prev)
dim(dz)
# Back-propagation
gradients <- back_propagation(NN, z.fw, y.fw, Y.train)
weight.delta <- learning_rate * gradients[[l]] +
momentum * (NN$weights[[l]] - old_weight[[l]])
weight.delta
weight.delta <- learning_rate * gradients[[l]] +
momentum * (NN$weights[[l]] - old_weight[[l]])
weight.delta
old_weight[[l]] <- NN$weights[[l]]
source("NN_new.R")
source("visualize_training_results.R")
dataset <- read.csv("sine_wave_dataset_preprocess.csv")
raw_dataset <- read.csv("sine_wave_dataset.csv")
X <- as.matrix(dataset$X)
Y <- as.matrix(dataset$Y)
nn <- NN.create(X, Y, c(5,4,3,2))
epochs <- 10000
result <- NN.train(nn, X, Y, epochs = epochs, learning_rate = 0.01, momentum = 0.9)
source("NN.R")
source("visualize_training_results.R")
dataset <- read.csv("sine_wave_dataset_preprocess.csv")
raw_dataset <- read.csv("sine_wave_dataset.csv")
X <- as.matrix(dataset$X)
Y <- as.matrix(dataset$Y)
nn <- NN.create(X, Y, c(5,4,3,2))
epochs <- 10000
result <- NN.train(nn, X, Y, epochs = epochs, learning_rate = 0.01, momentum = 0.9)
rm(list = ls())
clear
